NAME: Jeffrey Chan
EMAIL: jeffschan97@gmail.com
ID: 004-611-638

Answer to Questions:

2.1.1 - Causing Conflicts
The reason why it takes so many iterations before errors are seen is because a
larger number of iterations increases the chances of race conditions within the
critical sections of the counter code. The reason why having a significantly
smaller number of iterations seldom fails is because the chances of race
conditions within the critical sections is significantly lower because we're
not running the critical section as much.

2.1.2 - Cost of Yielding
The reason why the --yield runs are so much slower is because the yield option
forces the threads to yield the CPU once before they can continue to increment
or decrement the counter. Yielding the CPU means that a context switch needs to
occur so that another thread can run on the CPU. All the additional time is
going into context switching because contexting switching is and expensive
opperation. No, it's not possible to get valid per-operation timings when using
the --yield option because the large amount of time spent on performing a
context switch is included in the running time. Thus the total time of the
program doesn't accurately reflect time the program took to run its operations.

2.1.3 - Measurement Errors
When the number of iterations is small, the run time of the program is
dominated by the thread creation, and thus the average cost per operation is
perceived as higher because we weigh in thread creation time. As the number
of iterations increases, thread creation time becomes less significant and most
of the program's run time is spent actually doing the operations, and thus
the average cost per operation is perceived as lower. Thus, we see that the
cost per operation is inversely proportional to the number of iterations.
Theoretically, we can obtain the "correct" cost by tending the number of
iterations towards infinity, or just a really large number.

2.1.4 - Costs of Serialization
For a low number of threads, all the options perform similarly because the
program makes use of multiple CPU's. The number of threads is small enough such
that each thread can have its own CPU and thus run without having to wait to
obtain the lock to run critical code. Thus all the options with a low number
of threads act similarly in that their threads don't have to waste time waiting
for the lock and can execute critical code sequentially on different CPU's.
As the number of threads rises, there will not be enough CPU's for each thread,
so some threads will have to waste their time slice waiting for the lock,
which dramatically slows down the run time.

2.2.1 - Scalability of Mutex
Comparing the plots for the add program and the list program, both mutex curves
seem to flatten out as the number of threads increases. This is because
mutexes scale really well with increasing number of threads. When locking a
critical section, mutexes put threads to sleep so that they don't waste time
spinning. Therefore, the curves flattening out as the number of threads
increase make sense. The only difference is that in the add curve, the
average cost per opration increase from 1 thread to 4 threads, but this is
probably becasue within the add program, the cost of thread creation weighs
into the overall cost and causes this initial rise.

2.2.2 - Scalability of Spin Locks
Comparing the plots for the dd program and the list program, both
spin lock curves show a general linear increase as the number of threads
increases. As the number of threads increases, it just means that there are
more threads that wastes the CPU's by spinning when it doesn't have the lock.
Therefore the average cost per operation should be linearly proportional to
the number of threads, and our plots make sense. There are no notable
differences to note about the two plots.

Included Files:

Makefile: supports a build, tests, graphs, clean, dist target that does what is
specified in the spec.

README: file that contains answer to the questions, included files, and usage
of slip days.

SortedList.c: file that continas the implementations of SortedList methods,
implemented as a circularly linked list

SortedList.h: Definition of a SortedList_t and SortedListElement_t

lab2_add-[1-5].png: plots for lab2_add data

lab2_add.c: program that increments a counter and supports the arguments
specified in the spec

lab2_add.csv: test data used to form gnuplots

lab2_add.gp: gnuplot script used to generate the plots

lab2_list-[1-4].png: plots for lab2_list data

lab2_list.c: program that inserts, gets length, lookup, and deletes notes
from a sorted circularly linked list. Supports options as specified in the
spec

lab2_list.csv: test data used to generate the gnuplots

lab2_list.gp: gnuplot script used to generate the plots
